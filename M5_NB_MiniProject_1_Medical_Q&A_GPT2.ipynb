{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/imranahmed123/DataScience-AI-ML/blob/main/M5_NB_MiniProject_1_Medical_Q%26A_GPT2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hNgLag1Euy3H"
      },
      "source": [
        "# Advanced Certification Program in Computational Data Science\n",
        "## A programme by IISc and TalentSprint\n",
        "### Mini-Project: Medical Q&A using GPT2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-tdtrlAhvIHY"
      },
      "source": [
        "## Learning Objectives\n",
        "\n",
        "At the end of the experiment, you will be able to:\n",
        "\n",
        "* perform data preprocessing, EDA and feature extraction on the Medical Q&A dataset\n",
        "* load a pre-trained tokenizer\n",
        "* finetune a GPT-2 language model for medical question-answering"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset Description\n",
        "\n",
        "The dataset used in this project is the *Medical Question Answering Dataset* ([MedQuAD](https://github.com/abachaa/MedQuAD/tree/master)). It includes medical question-answer pairs along with additional information, such as the question type, the question *focus*, its UMLS(Unified Medical Language System) details like - Concept Unique Identifier(*CUI*) and Semantic *Type* and *Group*.\n",
        "\n",
        "To know more about this data's collection, and construction method, refer to this [paper](https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-019-3119-4).\n",
        "\n",
        "The data is extracted and is in CSV format with below features:\n",
        "\n",
        "- **Focus**: the question focus\n",
        "- **CUI**: concept unique identifier\n",
        "- **SemanticType**\n",
        "- **SemanticGroup**\n",
        "- **Question**\n",
        "- **Answer**"
      ],
      "metadata": {
        "id": "2nmP8OaXuO--"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R7ZV5ryMP07f"
      },
      "source": [
        "## Part-A: Grading = 10 Points"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Pped-3NPaDV"
      },
      "source": [
        "## Information"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Healthcare professionals often have to refer to medical literature and documents while seeking answers to medical queries. Medical databases or search engines are powerful resources of upto date medical knowledge. However, the existing documentation is large and makes it difficult for professionals to retrieve answers quickly in a clinical setting. The problem with search engines and informative retrieval engines is that these systems return a list of documents rather than answers. Instead, healthcare professionals can use question answering systems to retrieve short sentences or paragraphs in response to medical queries. Such systems have the biggest advantage of generating answers and providing hints in a few seconds."
      ],
      "metadata": {
        "id": "dW4w8T3jEPhi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Problem Statement"
      ],
      "metadata": {
        "id": "YN15WtbfwPkB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fine-tune gpt2 model on medical-question-answering-dataset for performing response generation for medical queries.\n",
        "\n",
        "Please refer to ***M6 Assignment-1 Fine-tune GPT2*** to get familiar with how to load pre-trained gpt2 tokenizer and model."
      ],
      "metadata": {
        "id": "R8_CoX7swR14"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9RH8Ecq9sbYU"
      },
      "source": [
        "### Import required packages"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install -U accelerate\n",
        "!pip -q install -U transformers\n",
        "!pip -q install torch"
      ],
      "metadata": {
        "id": "74vfKQ0RtRkR"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel, TextDataset, DataCollatorForLanguageModeling\n",
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "JfqnHAnMYfWF"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Download the dataset\n",
        "!wget -q https://cdn.iisc.talentsprint.com/AIandMLOps/MiniProjects/Datasets/MedQuAD.csv\n",
        "!ls | grep \".csv\""
      ],
      "metadata": {
        "cellView": "form",
        "id": "G0_3Qr2nI7B6",
        "outputId": "13729830-0cff-44df-e710-11398332a4e9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MedQuAD.csv\n",
            "MedQuAD.csv.1\n",
            "MedQuAD.csv.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T3FkLI6wcaat"
      },
      "source": [
        "**Exercise 1: Read the MedQuAD.csv dataset**\n",
        "\n",
        "**Hint:** pd.read_csv()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"MedQuAD.csv\")\n",
        "df.shape"
      ],
      "metadata": {
        "id": "m8Uwi8cYJ0Ia",
        "outputId": "9dcaa191-23f1-4a49-b1d1-2d8a1284e59a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(16412, 6)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "P-J6gSL_J9kQ",
        "outputId": "883708fb-0a1b-4c1a-f7d1-6757801329bc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                Focus       CUI SemanticType SemanticGroup  \\\n",
              "0  Adult Acute Lymphoblastic Leukemia  C0751606         T191     Disorders   \n",
              "1  Adult Acute Lymphoblastic Leukemia  C0751606         T191     Disorders   \n",
              "2  Adult Acute Lymphoblastic Leukemia  C0751606         T191     Disorders   \n",
              "3  Adult Acute Lymphoblastic Leukemia  C0751606         T191     Disorders   \n",
              "4  Adult Acute Lymphoblastic Leukemia  C0751606         T191     Disorders   \n",
              "\n",
              "                                            Question  \\\n",
              "0  What is (are) Adult Acute Lymphoblastic Leukem...   \n",
              "1  What are the symptoms of Adult Acute Lymphobla...   \n",
              "2  How to diagnose Adult Acute Lymphoblastic Leuk...   \n",
              "3  What is the outlook for Adult Acute Lymphoblas...   \n",
              "4  Who is at risk for Adult Acute Lymphoblastic L...   \n",
              "\n",
              "                                              Answer  \n",
              "0  Key Points - Adult acute lymphoblastic leukemi...  \n",
              "1  Signs and symptoms of adult ALL include fever,...  \n",
              "2  Tests that examine the blood and bone marrow a...  \n",
              "3  Certain factors affect prognosis (chance of re...  \n",
              "4  Previous chemotherapy and exposure to radiatio...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e6f5db3e-2f56-413d-a06a-d0d3bab2ad87\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Focus</th>\n",
              "      <th>CUI</th>\n",
              "      <th>SemanticType</th>\n",
              "      <th>SemanticGroup</th>\n",
              "      <th>Question</th>\n",
              "      <th>Answer</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Adult Acute Lymphoblastic Leukemia</td>\n",
              "      <td>C0751606</td>\n",
              "      <td>T191</td>\n",
              "      <td>Disorders</td>\n",
              "      <td>What is (are) Adult Acute Lymphoblastic Leukem...</td>\n",
              "      <td>Key Points - Adult acute lymphoblastic leukemi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Adult Acute Lymphoblastic Leukemia</td>\n",
              "      <td>C0751606</td>\n",
              "      <td>T191</td>\n",
              "      <td>Disorders</td>\n",
              "      <td>What are the symptoms of Adult Acute Lymphobla...</td>\n",
              "      <td>Signs and symptoms of adult ALL include fever,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Adult Acute Lymphoblastic Leukemia</td>\n",
              "      <td>C0751606</td>\n",
              "      <td>T191</td>\n",
              "      <td>Disorders</td>\n",
              "      <td>How to diagnose Adult Acute Lymphoblastic Leuk...</td>\n",
              "      <td>Tests that examine the blood and bone marrow a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Adult Acute Lymphoblastic Leukemia</td>\n",
              "      <td>C0751606</td>\n",
              "      <td>T191</td>\n",
              "      <td>Disorders</td>\n",
              "      <td>What is the outlook for Adult Acute Lymphoblas...</td>\n",
              "      <td>Certain factors affect prognosis (chance of re...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Adult Acute Lymphoblastic Leukemia</td>\n",
              "      <td>C0751606</td>\n",
              "      <td>T191</td>\n",
              "      <td>Disorders</td>\n",
              "      <td>Who is at risk for Adult Acute Lymphoblastic L...</td>\n",
              "      <td>Previous chemotherapy and exposure to radiatio...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e6f5db3e-2f56-413d-a06a-d0d3bab2ad87')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-e6f5db3e-2f56-413d-a06a-d0d3bab2ad87 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-e6f5db3e-2f56-413d-a06a-d0d3bab2ad87');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-ced2b875-2dfe-444e-be7e-79144488ea5d\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-ced2b875-2dfe-444e-be7e-79144488ea5d')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-ced2b875-2dfe-444e-be7e-79144488ea5d button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 16412,\n  \"fields\": [\n    {\n      \"column\": \"Focus\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 5126,\n        \"samples\": [\n          \"Sexual Problems in Women\",\n          \"Essential tremor\",\n          \"Low Vision\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"CUI\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3329,\n        \"samples\": [\n          \"C1843173\",\n          \"C3536714\",\n          \"C0427480\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"SemanticType\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 14,\n        \"samples\": [\n          \"T037\",\n          \"T028\",\n          \"T191\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"SemanticGroup\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"Disorders\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Question\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 14984,\n        \"samples\": [\n          \"Is prostate cancer inherited ?\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Answer\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 15809,\n        \"samples\": [\n          \"Some people may experience relief from symptoms of indigestion by - eating several small, low-fat meals throughout the day at a slow pace - refraining from smoking - abstaining from consuming coffee, carbonated beverages, and alcohol - stopping use of medications that may irritate the stomach liningsuch as aspirin or anti-inflammatory drugs - getting enough rest - finding ways to decrease emotional and physical stress, such as relaxation therapy or yoga The doctor may recommend over-the-counter antacids or medications that reduce acid production or help the stomach move food more quickly into the small intestine. Many of these medications can be purchased without a prescription. Nonprescription medications should only be used at the dose and for the length of time recommended on the label unless advised differently by a doctor. Informing the doctor when starting a new medication is important. Antacids, such as Alka-Seltzer, Maalox, Mylanta, Rolaids, and Riopan, are usually the first drugs recommended to relieve symptoms of indigestion. Many brands on the market use different combinations of three basic saltsmagnesium, calcium, and aluminumwith hydroxide or bicarbonate ions to neutralize the acid in the stomach. Antacids, however, can have side effects. Magnesium salt can lead to diarrhea, and aluminum salt may cause constipation. Aluminum and magnesium salts are often combined in a single product to balance these effects. Calcium carbonate antacids, such as Tums, Titralac, and Alka-2, can also be a supplemental source of calcium, though they may cause constipation. H2 receptor antagonists (H2RAs) include ranitidine (Zantac), cimetidine (Tagamet), famotidine (Pepcid), and nizatidine (Axid) and are available both by prescription and over-the-counter. H2RAs treat symptoms of indigestion by reducing stomach acid. They work longer than but not as quickly as antacids. Side effects of H2RAs may include headache, nausea, vomiting, constipation, diarrhea, and unusual bleeding or bruising. Proton pump inhibitors (PPIs) include omeprazole (Prilosec, Zegerid), lansoprazole (Prevacid), pantoprazole (Protonix), rabeprazole (Aciphex), and esomeprazole (Nexium) and are available by prescription. Prilosec is also available in over-the-counter strength. PPIs, which are stronger than H2RAs, also treat indigestion symptoms by reducing stomach acid. PPIs are most effective in treating symptoms of indigestion in people who also have GERD. Side effects of PPIs may include back pain, aching, cough, headache, dizziness, abdominal pain, gas, nausea, vomiting, constipation, and diarrhea. Prokinetics such as metoclopramide (Reglan) may be helpful for people who have a problem with the stomach emptying too slowly. Metoclopramide also improves muscle action in the digestive tract. Prokinetics have frequent side effects that limit their usefulness, including fatigue, sleepiness, depression, anxiety, and involuntary muscle spasms or movements. If testing shows the type of bacteria that causes peptic ulcer disease, the doctor may prescribe antibiotics to treat the condition.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QvddL7X69NiB"
      },
      "source": [
        "### Pre-processing and EDA"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise 2: Perform below operations on the dataset [0.5 Mark]**\n",
        "\n",
        "- Handle missing values\n",
        "- Remove duplicates from data considering `Question` and `Answer` columns"
      ],
      "metadata": {
        "id": "xgB7KCn88Vmj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Handle missing values**"
      ],
      "metadata": {
        "id": "TkabbxoGKFkm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# YOUR CODE HERE"
      ],
      "metadata": {
        "id": "H6mM7DM1KH_i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop missing values\n",
        "# YOUR CODE HERE\n"
      ],
      "metadata": {
        "id": "uNyjYvNu_48E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Handle missing values\n",
        "# Drop rows with missing values in 'Question' and 'Answer' columns\n",
        "df_cleaned = df.dropna(subset=['Question', 'Answer'])\n",
        "\n",
        "# Check if any missing values remain\n",
        "print(df_cleaned.isnull().sum())\n"
      ],
      "metadata": {
        "id": "XWuVXWuP4yyz",
        "outputId": "8247b39a-2650-4994-dace-9435a6e54f0c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Focus             14\n",
            "CUI              565\n",
            "SemanticType     597\n",
            "SemanticGroup    565\n",
            "Question           0\n",
            "Answer             0\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Remove duplicates from data considering `Question` and `Answer` columns**"
      ],
      "metadata": {
        "id": "0KpssAWUBiok"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check duplicates\n",
        "# YOUR CODE HERE"
      ],
      "metadata": {
        "id": "YEv2TMQPs-Ku"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop duplicates\n",
        "# YOUR CODE HERE"
      ],
      "metadata": {
        "id": "EYg8zF7itkf3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check duplicates\n",
        "# YOUR CODE HERE"
      ],
      "metadata": {
        "id": "5haFMucZtwiy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for duplicates based on 'Question' and 'Answer'\n",
        "duplicates_before = df.duplicated(subset=['Question', 'Answer']).sum()\n",
        "print(f\"Number of duplicates before removal: {duplicates_before}\")\n",
        "\n",
        "# Drop duplicates based on 'Question' and 'Answer'\n",
        "df_cleaned = df.drop_duplicates(subset=['Question', 'Answer'])\n",
        "\n",
        "# Check for duplicates again after removal\n",
        "duplicates_after = df_cleaned.duplicated(subset=['Question', 'Answer']).sum()\n",
        "print(f\"Number of duplicates after removal: {duplicates_after}\")\n"
      ],
      "metadata": {
        "id": "HMNOzn6i5Nmi",
        "outputId": "fec2e67e-5f6e-4b03-e745-96fe745d4f73",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of duplicates before removal: 48\n",
            "Number of duplicates after removal: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EA1d25HrzTGW"
      },
      "source": [
        "**Exercise 3: Display the category name, and the number of records belonging to top 100 categories of `Focus` column [1 Mark]**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# YOUR CODE HERE"
      ],
      "metadata": {
        "id": "kRxuMduGSWYB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Top 100 Focus categories names\n",
        "# YOUR CODE HERE"
      ],
      "metadata": {
        "id": "_Xa8jp6AKVkC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Group by the 'Focus' column and count the number of records for each category\n",
        "focus_category_counts = df_cleaned['Focus'].value_counts()\n",
        "\n",
        "# Display the top 100 'Focus' categories along with their counts\n",
        "top_100_focus_categories = focus_category_counts.head(100)\n",
        "\n",
        "# Print the top 100 categories\n",
        "print(top_100_focus_categories)\n"
      ],
      "metadata": {
        "id": "8pgFtlA15mHj",
        "outputId": "f62636ea-f8d6-4197-ca02-d751df285a98",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Focus\n",
            "Breast Cancer                                     53\n",
            "Prostate Cancer                                   43\n",
            "Stroke                                            35\n",
            "Skin Cancer                                       34\n",
            "Alzheimer's Disease                               30\n",
            "                                                  ..\n",
            "Poland syndrome                                   11\n",
            "Opitz G/BBB syndrome                              11\n",
            "Polycythemia Vera                                 11\n",
            "Diabetic Kidney Disease                           10\n",
            "What I need to know about Gestational Diabetes    10\n",
            "Name: count, Length: 100, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create Training and Validation set"
      ],
      "metadata": {
        "id": "clJZkJLzwDWR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise 4: Create training and validation set [2 Marks]**\n",
        "\n",
        "- Consider 4 samples per `Focus` category, for each top 100 categories, from the dataset (It will give 400 samples for training)\n",
        "\n",
        "- Consider 1 sample per `Focus` category (different from training set), for each top 100 categories, from the dataset (It will give 100 samples for validation)"
      ],
      "metadata": {
        "id": "JDQOWgthKgGG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# YOUR CODE HERE"
      ],
      "metadata": {
        "id": "CWoHsbibKtdo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a dictionary to store the samples for training and validation\n",
        "training_samples = []\n",
        "validation_samples = []\n",
        "\n",
        "# Loop through the top 100 focus categories\n",
        "for category in top_100_focus_categories.index:\n",
        "    # Filter rows belonging to the current category\n",
        "    category_rows = df_cleaned[df_cleaned['Focus'] == category]\n",
        "\n",
        "    # If there are more than 5 rows for this category, we can select 4 for training and 1 for validation\n",
        "    if len(category_rows) >= 5:\n",
        "        # Randomly sample 4 rows for training\n",
        "        train_sample = category_rows.sample(n=4, random_state=42)\n",
        "\n",
        "        # Remove the sampled rows from the remaining category rows\n",
        "        remaining_rows = category_rows.drop(train_sample.index)\n",
        "\n",
        "        # Randomly select 1 row for validation from the remaining rows\n",
        "        val_sample = remaining_rows.sample(n=1, random_state=42)\n",
        "\n",
        "        # Append these samples to the respective lists\n",
        "        training_samples.append(train_sample)\n",
        "        validation_samples.append(val_sample)\n",
        "\n",
        "# Concatenate the training and validation samples into separate DataFrames\n",
        "training_set = pd.concat(training_samples)\n",
        "validation_set = pd.concat(validation_samples)\n",
        "\n",
        "# Verify the sizes of training and validation sets\n",
        "print(f\"Training set size: {training_set.shape}\")\n",
        "print(f\"Validation set size: {validation_set.shape}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jPFqLoar5yrE",
        "outputId": "89fc15fe-00ac-4838-da5d-de1ebc296fc3"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set size: (400, 6)\n",
            "Validation set size: (100, 6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pre-process `Question` and `Answer` text\n",
        "\n",
        "**Exercise 5: Perform below tasks: [1.5 Marks]**\n",
        "\n",
        "- Combine `Question` and `Answer` for train and validation data as shown below:\n",
        "    - sequence = *'\\<question\\>' + question-text + '\\<answer\\>' + answer-text*\n",
        "\n",
        "- Join the combined text using '\\n' into a single string for training and validation separately\n",
        "\n",
        "- Save the training and validation strings as separate text files"
      ],
      "metadata": {
        "id": "YxjnEkqScZEJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Combine Question and Answer for train and val data**"
      ],
      "metadata": {
        "id": "48ieeFrU0vmF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# YOUR CODE HERE"
      ],
      "metadata": {
        "id": "nxT84SYwONaR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to combine 'Question' and 'Answer' columns for a given dataset\n",
        "def combine_question_answer(df):\n",
        "    combined_text = df.apply(lambda row: f\"<question>{row['Question']}<answer>{row['Answer']}\", axis=1)\n",
        "    # Join all the rows using '\\n' to create a single string\n",
        "    return '\\n'.join(combined_text)\n",
        "\n",
        "# Combine Question and Answer for training data\n",
        "train_sequence = combine_question_answer(training_set)\n",
        "\n",
        "# Combine Question and Answer for validation data\n",
        "val_sequence = combine_question_answer(validation_set)\n",
        "\n",
        "# Save the combined sequences as text files\n",
        "# Since this is for Google Colab, we will save the files locally\n",
        "with open('training_data.txt', 'w') as train_file:\n",
        "    train_file.write(train_sequence)\n",
        "\n",
        "with open('validation_data.txt', 'w') as val_file:\n",
        "    val_file.write(val_sequence)\n",
        "\n",
        "# Output file paths (for Google Colab use, these can be downloaded)\n",
        "print(\"Training data saved as: training_data.txt\")\n",
        "print(\"Validation data saved as: validation_data.txt\")\n"
      ],
      "metadata": {
        "id": "GHQGDAns7rhG",
        "outputId": "a62feadb-3d4c-4bf4-ad04-72d2ebd80071",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training data saved as: training_data.txt\n",
            "Validation data saved as: validation_data.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Join the combined text using '\\n' into a single string for training and validation separately**"
      ],
      "metadata": {
        "id": "Uzny8Cci1ZOw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# YOUR CODE HERE"
      ],
      "metadata": {
        "id": "vwKbaMAfPCM_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to combine 'Question' and 'Answer' into the desired format\n",
        "def combine_question_answer(df):\n",
        "    combined_text = df.apply(lambda row: f\"<question>{row['Question']}<answer>{row['Answer']}\", axis=1)\n",
        "    # Join all combined rows into a single string, separated by '\\n'\n",
        "    return '\\n'.join(combined_text)\n",
        "\n",
        "# Combine Question and Answer for the training data\n",
        "train_sequence = combine_question_answer(training_set)\n",
        "\n",
        "# Combine Question and Answer for the validation data\n",
        "val_sequence = combine_question_answer(validation_set)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_f4run1W8LTv"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Save the training and validation strings as text files**"
      ],
      "metadata": {
        "id": "GJZ9GHGSwNtm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# YOUR CODE HERE"
      ],
      "metadata": {
        "id": "pLQaAIhYMRvj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the combined sequences as text files in Google Colab\n",
        "with open('training_data_sequence.txt', 'w') as train_file:\n",
        "    train_file.write(train_sequence)\n",
        "\n",
        "with open('validation_data_sequence.txt', 'w') as val_file:\n",
        "    val_file.write(val_sequence)\n",
        "\n",
        "# Output to indicate where the files are saved in Google Colab\n",
        "print(\"Training data saved as: training_data_sequence.txt\")\n",
        "print(\"Validation data saved as: validation_data_sequence.txt\")"
      ],
      "metadata": {
        "id": "t8uKNx7N2lC0",
        "outputId": "34c1237a-e60f-4eaf-97cf-05f43fa1edc3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training data saved as: training_data_sequence.txt\n",
            "Validation data saved as: validation_data_sequence.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download('training_data_sequence.txt')\n",
        "files.download('validation_data_sequence.txt')"
      ],
      "metadata": {
        "id": "_3cixuf72l2a",
        "outputId": "9035c760-29f9-4f7e-d4bc-3ff6f31d20b0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_bc62d1a8-c4be-46b9-98f9-01fc2f044993\", \"training_data_sequence.txt\", 559537)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_fa7b3e8c-22b1-4e28-856c-6d6cc3a73b8d\", \"validation_data_sequence.txt\", 195852)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise 6: Load pre-trained GPT2Tokenizer [0.5 Mark]**\n",
        "\n",
        "- Use checkpoint = \"gpt2\""
      ],
      "metadata": {
        "id": "Y1hlYtjjML1r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# YOUR CODE HERE"
      ],
      "metadata": {
        "id": "-qiMe9TAplyj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the necessary module from the transformers library\n",
        "from transformers import GPT2Tokenizer\n",
        "\n",
        "# Load the pre-trained GPT2 tokenizer\n",
        "checkpoint = \"gpt2\"\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(checkpoint)\n",
        "\n",
        "# Print a message to confirm the tokenizer has been loaded\n",
        "print(f\"GPT-2 tokenizer loaded from checkpoint: {checkpoint}\")\n"
      ],
      "metadata": {
        "id": "j7t_gKvH89uK",
        "outputId": "0adc36e5-3b59-4d79-ca7c-24c97e702eb8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPT-2 tokenizer loaded from checkpoint: gpt2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise 7: Tokenize train and validation data and form TextDataset objects [0.5 Mark]**\n",
        "\n",
        "- Use the loaded pre-trained tokenizer\n",
        "- Use training and validation data saved in text files"
      ],
      "metadata": {
        "id": "InX4FOvgP0mi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# YOUR CODE HERE"
      ],
      "metadata": {
        "id": "crjMEbLfVOpq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the pad_token to eos_token (end-of-sequence) for padding purposes\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Function to load and tokenize text data\n",
        "def load_and_tokenize_data(file_path, tokenizer, block_size=512):\n",
        "    # Load the text data from the file\n",
        "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        text = f.read()\n",
        "\n",
        "    # Tokenize the text data using the pre-trained tokenizer\n",
        "    tokenized_data = tokenizer(text, return_tensors=\"pt\", max_length=block_size, truncation=True, padding=True)\n",
        "\n",
        "    return tokenized_data\n",
        "\n",
        "# Paths to the saved training and validation data\n",
        "train_file_path = \"training_data_sequence.txt\"\n",
        "val_file_path = \"validation_data_sequence.txt\"\n",
        "\n",
        "# Load and tokenize training and validation data\n",
        "train_data = load_and_tokenize_data(train_file_path, tokenizer)\n",
        "val_data = load_and_tokenize_data(val_file_path, tokenizer)\n",
        "\n",
        "# Print the size of the datasets to confirm\n",
        "print(f\"Training data tokenized: {len(train_data['input_ids'])} samples\")\n",
        "print(f\"Validation data tokenized: {len(val_data['input_ids'])} samples\")\n",
        "\n",
        "# Form TextDataset objects (using the tokenized inputs)\n",
        "train_dataset = TextDataset(tokenizer=tokenizer, file_path=train_file_path, block_size=512)\n",
        "val_dataset = TextDataset(tokenizer=tokenizer, file_path=val_file_path, block_size=512)\n",
        "\n",
        "# Print to confirm dataset creation\n",
        "print(\"Training and validation datasets created successfully.\")\n"
      ],
      "metadata": {
        "id": "wJ3TAPP29cNM",
        "outputId": "d6a390b7-d89f-447c-cbf5-5d22774feb2f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training data tokenized: 1 samples\n",
            "Validation data tokenized: 1 samples\n",
            "Training and validation datasets created successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise 8: Create a DataCollator object [0.5 Mark]**"
      ],
      "metadata": {
        "id": "QjN3cYyMkV74"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# YOUR CODE HERE"
      ],
      "metadata": {
        "id": "Z_XWmIF3cmhU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary module from transformers\n",
        "from transformers import DataCollatorForLanguageModeling\n",
        "\n",
        "# Create a DataCollator object for language modeling with GPT-2\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer,  # Use the pre-trained GPT-2 tokenizer\n",
        "    mlm=False  # Set to False because GPT-2 is not trained with masked language modeling\n",
        ")\n",
        "\n",
        "# Print a message to confirm the DataCollator creation\n",
        "print(\"DataCollatorForLanguageModeling created successfully.\")\n"
      ],
      "metadata": {
        "id": "bTQ9Pp0i9tlZ",
        "outputId": "f7dd5d18-e2f0-4f75-e89e-439caf559647",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DataCollatorForLanguageModeling created successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4NLnsjWF9te5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise 9: Load pre-trained GPT2LMHeadModel [0.5 Mark]**"
      ],
      "metadata": {
        "id": "uophCXjYq9MO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# YOUR CODE HERE"
      ],
      "metadata": {
        "id": "HxQWgssCqy7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the GPT2LMHeadModel from the transformers library\n",
        "from transformers import GPT2LMHeadModel\n",
        "\n",
        "# Load the pre-trained GPT2LMHeadModel using the checkpoint \"gpt2\"\n",
        "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "\n",
        "# Print a message to confirm the model has been loaded successfully\n",
        "print(\"GPT2LMHeadModel loaded successfully.\")\n"
      ],
      "metadata": {
        "id": "J179oaBx930q",
        "outputId": "8465b299-34d2-4ceb-fa39-fa4145a7e4f0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPT2LMHeadModel loaded successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise 10: Fine-tune GPT2 Model [1 Mark]**\n",
        "\n",
        "- Specify training arguments and create a TrainingArguments object (Use 30 epochs)\n",
        "\n",
        "- Train a GPT-2 model using the provided training arguments\n",
        "\n",
        "- Save the resulting trained model and tokenizer to a specified output directory"
      ],
      "metadata": {
        "id": "LdcpMx9QOPnU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up the training arguments\n",
        "\n",
        "# YOUR CODE HERE"
      ],
      "metadata": {
        "id": "UPZiEvn2cuPR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "# YOUR CODE HERE\n",
        "\n",
        "# Save the model\n",
        "# YOUR CODE HERE\n",
        "\n",
        "# Save the tokenizer\n",
        "# YOUR CODE HERE"
      ],
      "metadata": {
        "id": "drF9-oZwQLYl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# crashing for all available RAM memory\n",
        "# Import necessary modules\n",
        "#Try 1:\n",
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "# Set up the optimized training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"gpt2_fine_tuned\",  # Output directory\n",
        "    overwrite_output_dir=True,              # Overwrite the output directory\n",
        "    num_train_epochs=5,                     # Reduced number of epochs\n",
        "    per_device_train_batch_size=8,          # Increased batch size (if memory allows)\n",
        "    per_device_eval_batch_size=8,           # Increased eval batch size\n",
        "    warmup_steps=20,                        # Reduced number of warmup steps\n",
        "    weight_decay=0.01,                      # Weight decay\n",
        "    logging_dir=\"logs\",                     # Directory for storing logs\n",
        "    logging_steps=100,                      # Log every 100 steps (less frequent)\n",
        "    evaluation_strategy=\"epoch\",            # Evaluate at the end of each epoch\n",
        "    save_strategy=\"epoch\",                  # Save at the end of each epoch\n",
        "    fp16=True,                              # Use mixed precision for faster training\n",
        "    gradient_accumulation_steps=4,          # Accumulate gradients over 4 batches\n",
        ")\n",
        "\n",
        "# Create a Trainer object\n",
        "trainer = Trainer(\n",
        "    model=model,                            # The GPT-2 model\n",
        "    args=training_args,                     # Optimized training arguments\n",
        "    train_dataset=train_dataset,            # Training dataset\n",
        "    eval_dataset=val_dataset,               # Validation dataset\n",
        "    data_collator=data_collator,            # Data collator for dynamic padding\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "trainer.train()\n",
        "\n",
        "# Save the model\n",
        "trainer.save_model(\"gpt2_fine_tuned\")  # Save the fine-tuned model\n",
        "\n",
        "# Save the tokenizer\n",
        "tokenizer.save_pretrained(\"gpt2_fine_tuned\")  # Save the tokenizer\n",
        "\n",
        "# Print a message confirming that the model and tokenizer were saved\n",
        "print(\"Model and tokenizer saved successfully at gpt2_fine_tuned\")\n"
      ],
      "metadata": {
        "id": "IQ3mob9wTbDE",
        "outputId": "9fad9d67-4e32-4bf6-d9cc-e1049d6843f2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        }
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-07c0aee899f4>\u001b[0m in \u001b[0;36m<cell line: 32>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;31m# Save the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2050\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2051\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2052\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   2053\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2054\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2386\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2387\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccumulate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2388\u001b[0;31m                     \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2389\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2390\u001b[0m                 if (\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   3483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3484\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss_context_manager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3485\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3486\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3487\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mcompute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   3530\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3531\u001b[0m             \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3532\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3533\u001b[0m         \u001b[0;31m# Save past state if it exists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3534\u001b[0m         \u001b[0;31m# TODO: this needs to be fixed and made cleaner later.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1553\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1560\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1561\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/gpt2/modeling_gpt2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1314\u001b[0m         \u001b[0mreturn_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_return_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1315\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1316\u001b[0;31m         transformer_outputs = self.transformer(\n\u001b[0m\u001b[1;32m   1317\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m             \u001b[0mpast_key_values\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpast_key_values\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1553\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1560\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1561\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/gpt2/modeling_gpt2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1117\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient_checkpointing\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1118\u001b[0;31m                 outputs = self._gradient_checkpointing_func(\n\u001b[0m\u001b[1;32m   1119\u001b[0m                     \u001b[0mblock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1120\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_compile.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     29\u001b[0m                 \u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dynamo_disable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdisable_fn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mdisable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py\u001b[0m in \u001b[0;36m_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    598\u001b[0m             \u001b[0mprior\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset_eval_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    599\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 600\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    601\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m                 \u001b[0mset_eval_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprior\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py\u001b[0m in \u001b[0;36mcheckpoint\u001b[0;34m(function, use_reentrant, context_fn, determinism_check, debug, *args, **kwargs)\u001b[0m\n\u001b[1;32m    479\u001b[0m                 \u001b[0;34m\"use_reentrant=False.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    480\u001b[0m             )\n\u001b[0;32m--> 481\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mCheckpointFunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreserve\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    482\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    483\u001b[0m         gen = _checkpoint_without_reentrant_generator(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/function.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    572\u001b[0m             \u001b[0;31m# See NOTE: [functorch vjp and autograd interaction]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    573\u001b[0m             \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_functorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap_dead_wrappers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 574\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    575\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    576\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_setup_ctx_defined\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(ctx, run_function, preserve_rng_state, *args)\u001b[0m\n\u001b[1;32m    253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 255\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1553\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1560\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1561\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/gpt2/modeling_gpt2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    613\u001b[0m         \u001b[0mresidual\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    614\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mln_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 615\u001b[0;31m         attn_outputs = self.attn(\n\u001b[0m\u001b[1;32m    616\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    617\u001b[0m             \u001b[0mlayer_past\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlayer_past\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1553\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1560\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1561\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/gpt2/modeling_gpt2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    543\u001b[0m         \u001b[0mis_causal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mattention_mask\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mq_len\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_cross_attention\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 545\u001b[0;31m         attn_output = torch.nn.functional.scaled_dot_product_attention(\n\u001b[0m\u001b[1;32m    546\u001b[0m             \u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m             \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the data from your dataset (CSV, TXT, etc.)\n",
        "# This example assumes you have a CSV file with 'text' column\n",
        "df = pd.read_csv(\"MedQuAD.csv\")\n",
        "\n",
        "# Select only 100 samples for training and 20 for validation\n",
        "train_df = df.sample(n=100, random_state=42)\n",
        "val_df = df.sample(n=20, random_state=42)\n",
        "\n",
        "# Save these smaller datasets into temporary files for quick execution\n",
        "train_df.to_csv(\"train_subset.csv\", index=False)\n",
        "val_df.to_csv(\"val_subset.csv\", index=False)\n",
        "\n",
        "# Create TextDataset from the smaller files\n",
        "from transformers import TextDataset, DataCollatorForLanguageModeling\n",
        "\n",
        "# Load the reduced dataset into TextDataset\n",
        "train_dataset = TextDataset(\n",
        "    tokenizer=tokenizer,\n",
        "    file_path=\"train_subset.csv\",\n",
        "    block_size=512\n",
        ")\n",
        "\n",
        "val_dataset = TextDataset(\n",
        "    tokenizer=tokenizer,\n",
        "    file_path=\"val_subset.csv\",\n",
        "    block_size=512\n",
        ")\n",
        "\n",
        "# You can proceed with training now using these smaller datasets\n"
      ],
      "metadata": {
        "id": "ndh04Qf5BeLL"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary modules\n",
        "# Try2:\n",
        "from transformers import Trainer, TrainingArguments, GPT2LMHeadModel, GPT2Tokenizer\n",
        "\n",
        "# Load a smaller version of the GPT-2 model (distilgpt2)\n",
        "model = GPT2LMHeadModel.from_pretrained('distilgpt2')\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('distilgpt2')\n",
        "\n",
        "# Enable gradient checkpointing to reduce memory usage\n",
        "model.gradient_checkpointing_enable()\n",
        "\n",
        "# Reduce dataset size for quicker execution\n",
        "#train_dataset = train_dataset.select(range(100))  # Only 100 samples for training\n",
        "#val_dataset = val_dataset.select(range(20))       # Only 20 samples for validation\n",
        "\n",
        "# Set up the optimized training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"gpt2_fine_tuned\",  # Output directory\n",
        "    overwrite_output_dir=True,              # Overwrite the output directory\n",
        "    num_train_epochs=2,                     # Reduced number of epochs\n",
        "    per_device_train_batch_size=2,          # Reduced batch size to 2\n",
        "    per_device_eval_batch_size=2,           # Reduced eval batch size to 2\n",
        "    warmup_steps=20,                        # Reduced number of warmup steps\n",
        "    weight_decay=0.01,                      # Weight decay\n",
        "    logging_dir=\"logs\",                     # Directory for storing logs\n",
        "    logging_steps=100,                      # Log every 100 steps\n",
        "    evaluation_strategy=\"epoch\",            # Evaluate at the end of each epoch\n",
        "    save_strategy=\"epoch\",                  # Save at the end of each epoch\n",
        "    fp16=True,                              # Use mixed precision for faster training\n",
        "    gradient_accumulation_steps=4,          # Accumulate gradients over 4 batches\n",
        ")\n",
        "\n",
        "# Create a Trainer object\n",
        "trainer = Trainer(\n",
        "    model=model,                            # The GPT-2 model\n",
        "    args=training_args,                     # Optimized training arguments\n",
        "    train_dataset=train_dataset,            # Training dataset\n",
        "    eval_dataset=val_dataset,               # Validation dataset\n",
        "    data_collator=data_collator,            # Data collator for dynamic padding\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "trainer.train()\n",
        "\n",
        "# Save the model\n",
        "trainer.save_model(\"gpt2_fine_tuned\")  # Save the fine-tuned model\n",
        "\n",
        "# Save the tokenizer\n",
        "tokenizer.save_pretrained(\"gpt2_fine_tuned\")  # Save the tokenizer\n",
        "\n",
        "# Print a message confirming that the model and tokenizer were saved\n",
        "print(\"Model and tokenizer saved successfully at gpt2_fine_tuned\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 155
        },
        "id": "8FzbY7Mxdt4q",
        "outputId": "472d2af2-f7b7-471b-f3c8-7c6d2876c9b0"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='16' max='16' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [16/16 15:34, Epoch 2/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>2.878483</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>No log</td>\n",
              "      <td>2.725487</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model and tokenizer saved successfully at gpt2_fine_tuned\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VdImBkrd_Qjo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise 11: Test Model with user input prompts [1 Mark]**\n",
        "\n",
        "- Create `generate_response()` function that takes a trained *model*, *tokenizer*, and a *prompt* string as input and generates a response using the GPT-2 model\n",
        "\n",
        "- Test it with some user input prompts"
      ],
      "metadata": {
        "id": "4izo_-go8cDP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# YOUR CODE HERE"
      ],
      "metadata": {
        "id": "qeTKPArfgDJW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the fine-tuned model and tokenizer\n",
        "\n",
        "# YOUR CODE HERE"
      ],
      "metadata": {
        "id": "pJJ3bzD9fsJv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Response from model\n",
        "\n",
        "# YOUR CODE HERE"
      ],
      "metadata": {
        "id": "UCkZjYC_ht7r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing with given prompt 1\n",
        "\n",
        "# YOUR CODE HERE"
      ],
      "metadata": {
        "id": "DlVNC0LliaMK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing with given prompt 2\n",
        "\n",
        "# YOUR CODE HERE"
      ],
      "metadata": {
        "id": "WWOTby9IiyQ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary module from transformers\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "\n",
        "# Function to generate a response from the model\n",
        "def generate_response(model, tokenizer, prompt, max_length=50, num_return_sequences=1):\n",
        "    # Encode the prompt into input_ids for the model\n",
        "    inputs = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
        "\n",
        "    # Generate output using the fine-tuned model\n",
        "    outputs = model.generate(\n",
        "        inputs,\n",
        "        max_length=max_length,          # Maximum length of the generated sequence\n",
        "        num_return_sequences=num_return_sequences,  # Number of responses to generate\n",
        "        no_repeat_ngram_size=2,         # Avoid repetition of words\n",
        "        do_sample=True,                 # Sampling to introduce variability in response\n",
        "        top_k=50,                       # Number of highest probability words considered\n",
        "        top_p=0.95,                     # Cumulative probability for token selection\n",
        "        temperature=0.7                 # Controls randomness: lower is more conservative\n",
        "    )\n",
        "\n",
        "    # Decode the output to get the generated text\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    return response\n",
        "\n",
        "# Load the fine-tuned model and tokenizer\n",
        "# Assuming you have already fine-tuned and saved the model\n",
        "model = GPT2LMHeadModel.from_pretrained(\"gpt2_fine_tuned\")\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2_fine_tuned\")\n",
        "\n",
        "# Testing with given prompt 1\n",
        "prompt_1 = \"What are the symptoms of diabetes?\"\n",
        "response_1 = generate_response(model, tokenizer, prompt_1)\n",
        "print(f\"Prompt 1: {prompt_1}\\nResponse 1: {response_1}\\n\")\n",
        "\n",
        "# Testing with given prompt 2\n",
        "prompt_2 = \"How is hypertension treated?\"\n",
        "response_2 = generate_response(model, tokenizer, prompt_2)\n",
        "print(f\"Prompt 2: {prompt_2}\\nResponse 2: {response_2}\\n\")\n"
      ],
      "metadata": {
        "id": "U6zyN2jEAsJV",
        "outputId": "a77f2278-dcd6-4bd9-a7cf-af1b5fbe405f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt 1: What are the symptoms of diabetes?\n",
            "Response 1: What are the symptoms of diabetes? The term is often used to describe the common symptoms that cause diabetes. The following list of symptoms may be helpful to you.\n",
            "\n",
            "Diabetes\n",
            "People who have diabetes who are diagnosed with type 1 diabetes are more\n",
            "\n",
            "Prompt 2: How is hypertension treated?\n",
            "Response 2: How is hypertension treated?\n",
            "\n",
            "In the following papers, the authors (in this paper) examined the occurrence of hypertension in the U.S. population (U. S.M.) from the 1970s through 1990s. In their study,\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise 12: Compare the performance of a *GPT2 model* with the *GPT2 model fine-tuned* on MedQuAD data [1 Mark]**\n",
        "\n",
        "- Load another pre-trained GPT2LMHeadModel and do not fine-tune it\n",
        "\n",
        "- To generate response using the untuned model, pass it as a parameter to `generate_response()` function\n",
        "\n",
        "- Test both models (fine-tuned and untuned) with below user input prompts:\n",
        "\n",
        "    - \"What precautions to take for a healthy life?\"\n",
        "    - \"What to do after being diagnosed with cancer?\"\n",
        "    - \"What to do when feeling sick?\""
      ],
      "metadata": {
        "id": "NO_AdtZc8ThO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load a pre-trained GPT2 model, do not finetune it with MedQuAD data\n",
        "\n",
        "# YOUR CODE HERE"
      ],
      "metadata": {
        "id": "u7Inq43Q524A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing with finetuned model: prompt 1\n",
        "\n",
        "# YOUR CODE HERE"
      ],
      "metadata": {
        "id": "wb58jO6b4gC4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing with untuned model: prompt 1\n",
        "\n",
        "# YOUR CODE HERE"
      ],
      "metadata": {
        "id": "q3rN6ETD46PS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing with finetuned model: prompt 2\n",
        "\n",
        "# YOUR CODE HERE"
      ],
      "metadata": {
        "id": "Pl0ChnZn6XVq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing with untuned model: prompt 2\n",
        "\n",
        "# YOUR CODE HERE"
      ],
      "metadata": {
        "id": "gG4lVy3F6u2B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing with finetuned model: prompt 3\n",
        "\n",
        "# YOUR CODE HERE"
      ],
      "metadata": {
        "id": "yqu9MAci7dvN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing with untuned model: prompt 3\n",
        "\n",
        "# YOUR CODE HERE"
      ],
      "metadata": {
        "id": "jdj0En4o61Zj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 1: Load the pre-trained GPT-2 model (untuned)"
      ],
      "metadata": {
        "id": "mzZm7DEnIs4n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "\n",
        "# Load a pre-trained GPT2 model (untuned, not fine-tuned on MedQuAD)\n",
        "untuned_model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
        "untuned_tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n"
      ],
      "metadata": {
        "id": "XPEMx4ZsIvZD"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 2: Load the fine-tuned GPT-2 model"
      ],
      "metadata": {
        "id": "UW3AqNw6I3ba"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the fine-tuned model from the previous steps (MedQuAD fine-tuned GPT2)\n",
        "finetuned_model = GPT2LMHeadModel.from_pretrained('gpt2_fine_tuned')\n",
        "finetuned_tokenizer = GPT2Tokenizer.from_pretrained('gpt2_fine_tuned')\n"
      ],
      "metadata": {
        "id": "ctw-78NGI4NA"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 3: Generate responses using both models"
      ],
      "metadata": {
        "id": "NJXO5mHPI6ug"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_response(model, tokenizer, prompt, max_length=50):\n",
        "    # Encode the prompt into token IDs\n",
        "    inputs = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
        "\n",
        "    # Generate the output using the model\n",
        "    outputs = model.generate(\n",
        "        inputs,\n",
        "        max_length=max_length,\n",
        "        num_return_sequences=1,\n",
        "        no_repeat_ngram_size=2,   # Prevents repetition of words\n",
        "        do_sample=True,           # Sampling to generate diverse outputs\n",
        "        top_k=50,                 # Consider top 50 tokens for next word\n",
        "        top_p=0.95,               # Nucleus sampling to consider tokens until probability mass is 0.95\n",
        "        temperature=0.7           # Control randomness, lower means less random\n",
        "    )\n",
        "\n",
        "    # Decode the generated token IDs back into human-readable text\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n"
      ],
      "metadata": {
        "id": "J8TC5luXJE2R"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 4: Test both models with the three input prompts"
      ],
      "metadata": {
        "id": "6npjRJeGJL8x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prompt 1: \"What precautions to take for a healthy life?\""
      ],
      "metadata": {
        "id": "o2022spKJPx5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fine-tuned model response\n",
        "finetuned_response_1 = generate_response(finetuned_model, finetuned_tokenizer, \"What precautions to take for a healthy life?\")\n",
        "print(\"Fine-tuned model response 1:\", finetuned_response_1)\n",
        "\n",
        "# Untuned model response\n",
        "untuned_response_1 = generate_response(untuned_model, untuned_tokenizer, \"What precautions to take for a healthy life?\")\n",
        "print(\"Untuned model response 1:\", untuned_response_1)\n"
      ],
      "metadata": {
        "id": "pT1w3WZ8JQuo",
        "outputId": "cc734c0e-d5d3-4e9e-85dd-96cecf0e4f2e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fine-tuned model response 1: What precautions to take for a healthy life?\n",
            "\n",
            "The best advice for preventing a cancer is to never take a pill or an aspirin for about a week.\n",
            "You should take the pill regularly and not take it every day. You should also take\n",
            "Untuned model response 1: What precautions to take for a healthy life?\n",
            "\n",
            "Do not use or consume alcohol or tobacco.\n",
            ". Do not smoke or drink.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "n6HqkrfNJLeF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prompt 2: \"What to do after being diagnosed with cancer?\""
      ],
      "metadata": {
        "id": "QMjYL1ctJhKV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fine-tuned model response\n",
        "finetuned_response_2 = generate_response(finetuned_model, finetuned_tokenizer, \"What to do after being diagnosed with cancer?\")\n",
        "print(\"Fine-tuned model response 2:\", finetuned_response_2)\n",
        "\n",
        "# Untuned model response\n",
        "untuned_response_2 = generate_response(untuned_model, untuned_tokenizer, \"What to do after being diagnosed with cancer?\")\n",
        "print(\"Untuned model response 2:\", untuned_response_2)\n"
      ],
      "metadata": {
        "id": "dqSvJ7oPJkFS",
        "outputId": "8481cf8b-4c05-41ca-e122-4e2540e86de0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fine-tuned model response 2: What to do after being diagnosed with cancer? How much time do you have to spend with a cancer patient? The answer is 3-6 months.\n",
            "\n",
            "It's a simple question. There's no doctor's recommendation for what to take. It\n",
            "Untuned model response 2: What to do after being diagnosed with cancer?\n",
            "\n",
            "Take your time. Get it right when you can.\n",
            ",\n",
            ". See you later!\n",
            "\"Treatment of cancer is not based on treatment,\" said Dr. J.H. H\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prompt 3: \"What to do when feeling sick?\""
      ],
      "metadata": {
        "id": "hSnGAAHDJuZ9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fine-tuned model response\n",
        "finetuned_response_3 = generate_response(finetuned_model, finetuned_tokenizer, \"What to do when feeling sick?\")\n",
        "print(\"Fine-tuned model response 3:\", finetuned_response_3)\n",
        "\n",
        "# Untuned model response\n",
        "untuned_response_3 = generate_response(untuned_model, untuned_tokenizer, \"What to do when feeling sick?\")\n",
        "print(\"Untuned model response 3:\", untuned_response_3)\n"
      ],
      "metadata": {
        "id": "C21iy7ZSJvCR",
        "outputId": "234a4e32-7d37-4798-9508-baed59fa9d44",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fine-tuned model response 3: What to do when feeling sick? How do you feel?\n",
            "\n",
            "Here's a list of all the steps that you can take to get rid of your symptoms.\n",
            "1) Do your doctors have a recommendation for treatment? In order to learn about\n",
            "Untuned model response 3: What to do when feeling sick?\n",
            "\n",
            "It can take many forms. If you have severe stomach pain, you may need to go to the emergency room. However, it is usually best to avoid the doctor if you experience pain or aching throat\n"
          ]
        }
      ]
    }
  ]
}